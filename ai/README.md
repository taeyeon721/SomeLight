# AI - Deep Learning

ë”¥ëŸ¬ë‹ì€ ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µ ì‚¬ì´ì— ì—¬ëŸ¬ ê°œì˜ ì€ë‹‰ì¸µìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì‹ ê²½ë§ì´ë‹¤.

ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ëª¨ë“ˆê³¼ í•¨ìˆ˜ì— ë”°ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°(hyper-parameter)ë„ ë¹„ë¡€í•˜ì—¬ ë§ì•„ì§€ê¸°ì—,

ì´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê²°ì •í•˜ì—¬ ëª¨ë¸ì´ ì •í™•í•œ ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ í•™ìŠµì˜ í•µì‹¬ì´ë‹¤.

<aside>
ğŸ’¡ **í•™ìŠµ** : ê¸°ê³„ê°€ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì˜¤ì°¨ë¡œë¶€í„° optimizerë¥¼ í†µí•´ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •

</aside>

ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ì†ì‹¤ í•¨ìˆ˜(Loss Function)ì„ ì •ì˜í•´ì•¼ í•˜ë©° ì´ë¥¼ ìµœì í™”(Optimization)í•´ì•¼í•œë‹¤.

ì¸ê³µ ì‹ ê²½ë§ì˜ í•™ìŠµì€ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì°¾ëŠ” ëª©ì ìœ¼ë¡œ ìˆœì „íŒŒì™€ ì—­ì „íŒŒë¥¼ ë°˜ë³µí•˜ëŠ” ê²ƒì„ ë§í•œë‹¤.

## Tokenizer

## Loss Function

- ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ë¥¼ ìˆ˜ì¹˜í™”í•´ì£¼ëŠ” í•¨ìˆ˜
- ë‘ ê°’ì˜ ì°¨ì´(=ì˜¤ì°¨)ê°€ í´ ìˆ˜ë¡ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì€ í¬ê³ , ì˜¤ì°¨ê°€ ì‘ì„ ìˆ˜ë¡ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì€ ì‘ì•„ì§„ë‹¤.
- ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ë‘ ê°œì˜ ë§¤ê°œë³€ìˆ˜ì¸ ê°€ì¤‘ì¹˜ wì™€ í¸í–¥ bì˜ ê°’ì„ ì°¾ëŠ” ê²ƒì´ ë”¥ëŸ¬ë‹ì˜ í•™ìŠµ ê³¼ì •

**Categorical Cross-Entropy**

- ì‚¬ìš©í•  ì†ì‹¤ í•¨ìˆ˜
- ì¶œë ¥ì¸µì—ì„œ softmax í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜(Multi-Class Classification)ì¼ ê²½ìš° ì‚¬ìš©
- ë§Œì•½ ë ˆì´ë¸”ì— ëŒ€í•´ì„œ one-hot encodingì„ ìƒëµí•˜ê³ , ì •ìˆ˜ê°’ì„ ê°€ì§„ ë ˆì´ë¸”ì— ëŒ€í•´ì„œ ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ê³  ì‹¶ìœ¼ë©´ â€˜sparse_categorical_crossentropyâ€™ ì‚¬ìš©

```python
# categorical cross-entropy
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
# tensorflow
model.compile(lose=tf.keras.losses.CategoricalCrossentropy(), optimizer='adam', metrics=['acc'])

# sparse categorical cross-entropy
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])
# tensorflow
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=['acc'])
```

## Gradient Descent

- MLDL ì•Œê³ ë¦¬ì¦˜ì„ í•™ìŠµì‹œí‚¬ ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ 1ì°¨ ê·¼ì‚¿ê°’ ë°œê²¬ìš© ìµœì í™” ì•Œê³ ë¦¬ì¦˜
- í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°(ê²½ì‚¬)ë¥¼ êµ¬í•˜ì—¬ ê¸°ìš¸ê¸°ê°€ ë‚®ì€ ìª½ìœ¼ë¡œ ê³„ì† ì´ë™ì‹œì¼œ ê·¹ê°’(ìµœì ê°’)ì— ì´ë¥¼ ë•Œê¹Œì§€ ë°˜ë³µ
- ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ì¤„ì—¬ë‚˜ê°€ë©´ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²• ì¤‘ ë°°ì¹˜(Batch)ëŠ” ê°€ì¤‘ì¹˜ ë“±ì˜ ë§¤ê°œ ë³€ìˆ˜ì˜ ê°’ì„ ì¡°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì˜ ì–‘

**Mini-Batch Gradient Descent**

- ì‚¬ìš©í•  ê²½ì‚¬ í•˜ê°•ë²•
- ë°°ì¹˜ í¬ê¸°ë¥¼ ì§€ì •í•˜ì—¬ í•´ë‹¹ ë°ì´í„° ê°œìˆ˜ë§Œí¼ì— ëŒ€í•˜ì—¬ ê³„ì‚°í•˜ì—¬ ë§¤ê°œ ë³€ìˆ˜ì˜ ê°’ì„ ì¡°ì •í•˜ëŠ” ê²½ì‚¬ í•˜ê°•ë²•
- ë°°ì¹˜ í¬ê¸°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ 2ì˜ nì œê³±ì— í•´ë‹¹í•˜ëŠ” ìˆ«ìë¡œ ì„ íƒí•˜ëŠ” ê²ƒì´ ë³´í¸ì 
- batch_sizeì˜ defaultëŠ” 32

```python
model.fit(X_train, y_train, batch_size=128)
```

## Optimizer

- ìµœì í™”(Optimization)ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê²°ê³¼ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜)ë¥¼ ì°¾ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©°, OptimizerëŠ” ìµœì í™”ì˜ ì•Œê³ ë¦¬ì¦˜

**Adam**

- ì‚¬ìš©í•  ìµœì í™” ì•Œê³ ë¦¬ì¦˜
- RMSprop + Momentum, ë°©í–¥ê³¼ í•™ìŠµë¥  ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ ì¡ëŠ” ë°©ë²•

```python
# direct setting
adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])

# default adam
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
```

## Overfitting

- í•™ìŠµ ë°ì´í„°ì— ëª¨ë¸ì´ ê³¼ì í•©ë˜ëŠ” í˜„ìƒì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë–¨ì–´íŠ¸ë¦¬ëŠ” ì£¼ìš” ì´ìŠˆ
- ê³¼ì í•©ì„ ë§‰ëŠ” ë°©ë²•
    1. ë°ì´í„°ì˜ ì–‘ ëŠ˜ë¦¬ê¸°
    2. ëª¨ë¸ì˜ ë³µì¡ë„ ì¤„ì´ê¸°
    3. ê°€ì¤‘ì¹˜ ê·œì œ(Regularization) ì ìš©
        1. L1, L2
    4. **Dropout**
        1. ì‹ ê²½ë§ í•™ìŠµ ì‹œì—ë§Œ ì‚¬ìš©í•˜ê³ , ì˜ˆì¸¡ ì‹œì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¼ë°˜ì 
        
        ```python
        from tensorflow.keras.models import Sequential
        from tensorflow.keras.layers import Dropout, Dense
        
        max_words = 10000
        num_classes = 46
        
        model = Sequential()
        model.add(Dense(256, input_shape=(max_words,), activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(128, activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(num_classes, activation='softmax'))
        
        ```
        

## Weight Initialization

- Gradient Vanishing/Exploding ë§‰ëŠ” ë°©ë²•
- ê°™ì€ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ë”ë¼ë„ ê°€ì¤‘ì¹˜ê°€ ì´ˆê¸°ì— ì–´ë–¤ ê°’ì„ ê°€ì¡ŒëŠëƒì— ë”°ë¼ì„œ ëª¨ë¸ì˜ í›ˆë ¨ ê²°ê³¼ê°€ ë‹¬ë¼ì§€ê¸°ë„ í•œë‹¤.
    - ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ë§Œ ì ì ˆíˆ í•´ì¤˜ë„ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œì™€ ê°™ì€ ë¬¸ì œê°€ ì™„í™”ë¨
    1. Xavier Initialization
    2. He Initialization
- ReLU + HE Initialization ë°©ë²•ì´ íš¨ìœ¨ì 

### ì˜ˆì œ(MLP)

****20ê°œ ë‰´ìŠ¤ ê·¸ë£¹(Twenty Newsgroups) ë°ì´í„°ì— ëŒ€í•œ ì´í•´****

[https://wikidocs.net/49071](https://wikidocs.net/49071)

```python
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical

newsdata = fetch_20newsgroups(subset='train')
data = pd.DataFrame(newsdata.data, columns=['email'])
data['target']=pd.Series(newsdata.target)
data.info()

newsdata_test = fetch_20newsgroups(subset='test', shuffle=True)
train_email = data['email']
train_label=data['target']
test_email=newsdata_test.data
test_label=newsdata_test.target

vocab_size=10000
num_classes=20

def prepare_data(train_data, test_data, mode):
  tokenizer=Tokenizer(num_words=vocab_size)
  tokenizer.fit_on_texts(train_data)
  X_train=tokenizer.texts_to_matrix(train_data, mode=mode)
  X_test = tokenizer.texts_to_matrix(test_data, mode=mode)
  return X_train, X_test, tokenizer.index_word

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

def fit_and_evaluate(X_train, y_train, X_test, y_test):
  model = Sequential()
  model.add(Dense(256, input_shape=(vocab_size,), activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(128, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(num_classes, activation='softmax'))

  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  model.fit(X_train, y_train, batch_size=128, epochs=5, verbose=1, validation_split=0.1)
  score = model.evaluate(X_test, y_test, batch_size=128, verbose=0)
  return score[1]

modes = ['binary', 'count', 'tfidf', 'freq']

for mode in modes:
  X_train, X_test, _ = prepare_data(train_email, test_email, mode)
  score = fit_and_evaluate(X_train, y_train, X_test, y_test)
  print(mode+' ëª¨ë“œì˜ í…ŒìŠ¤íŠ¸ ì •í™•ë„: ', score)
```

| Epoch 1/5
80/80 [==============================] - 4s 40ms/step - loss: 2.2590 - accuracy: 0.3417 - val_loss: 0.9385 - val_accuracy: 0.8198
Epoch 2/5
80/80 [==============================] - 4s 45ms/step - loss: 0.8611 - accuracy: 0.7619 - val_loss: 0.4584 - val_accuracy: 0.8958
â€¦
Epoch 5/5
80/80 [==============================] - 3s 36ms/step - loss: 0.1664 - accuracy: 0.9584 - val_loss: 0.2924 - val_accuracy: 0.9161
binary ëª¨ë“œì˜ í…ŒìŠ¤íŠ¸ ì •í™•ë„:  0.8291290402412415
Epoch 1/5
80/80 [==============================] - 5s 49ms/step - loss: 2.8471 - accuracy: 0.2392 - val_loss: 1.6509 - val_accuracy: 0.7253
Epoch 2/5
80/80 [==============================] - 3s 36ms/step - loss: 1.4439 - accuracy: 0.6166 - val_loss: 0.7141 - val_accuracy: 0.8551
â€¦
Epoch 5/5
80/80 [==============================] - 3s 44ms/step - loss: 0.3810 - accuracy: 0.9159 - val_loss: 0.4007 - val_accuracy: 0.8993
count ëª¨ë“œì˜ í…ŒìŠ¤íŠ¸ ì •í™•ë„:  0.8158523440361023
Epoch 1/5
80/80 [==============================] - 4s 41ms/step - loss: 2.2124 - accuracy: 0.3645 - val_loss: 0.7934 - val_accuracy: 0.8277
Epoch 2/5
80/80 [==============================] - 4s 46ms/step - loss: 0.8355 - accuracy: 0.7733 - val_loss: 0.4380 - val_accuracy: 0.8905
â€¦
Epoch 5/5
80/80 [==============================] - 3s 40ms/step - loss: 0.2247 - accuracy: 0.9482 - val_loss: 0.3411 - val_accuracy: 0.9117
tfidf ëª¨ë“œì˜ í…ŒìŠ¤íŠ¸ ì •í™•ë„:  0.8313860893249512
Epoch 1/5
80/80 [==============================] - 5s 50ms/step - loss: 2.9785 - accuracy: 0.0940 - val_loss: 2.9296 - val_accuracy: 0.3401
Epoch 2/5
80/80 [==============================] - 3s 40ms/step - loss: 2.7296 - accuracy: 0.2117 - val_loss: 2.4050 - val_accuracy: 0.3534
â€¦
Epoch 5/5
80/80 [==============================] - 3s 41ms/step - loss: 1.4301 - accuracy: 0.5566 - val_loss: 1.2187 - val_accuracy: 0.6908
freq ëª¨ë“œì˜ í…ŒìŠ¤íŠ¸ ì •í™•ë„:  0.6566649079322815  |
| --- |

[ë”¥ ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/book/2155)